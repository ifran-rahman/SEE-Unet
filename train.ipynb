{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a GPU with ID: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), Name: NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# List all physical devices of type GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        gpu_details = tf.config.experimental.get_device_details(gpu)\n",
    "        gpu_name = gpu_details.get('device_name', 'Unknown GPU')\n",
    "        print(f\"Found a GPU with ID: {gpu}, Name: {gpu_name}\")\n",
    "else:\n",
    "    print(\"Failed to detect a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 318\n",
      "Number of training masks: 318\n",
      "Number of validation images: 35\n",
      "Number of validation masks: 35\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from networks.Sqeeze_Expand_Excite import SEE_Unet\n",
    "import random\n",
    "\n",
    "from networks.squeezeunet import SqueezeUNet\n",
    "from networks.unet3 import unet\n",
    "from networks.Sqeeze_Expand_Excite import SEE_Unet\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "# os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "def set_seeds(seed=seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Activate Tensorflow deterministic behavior\n",
    "def set_global_determinism(seed=seed):\n",
    "    set_seeds(seed=seed)\n",
    "\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_global_determinism(seed=seed)\n",
    "\n",
    "# -----------------             ---------------------------------------------------------------\n",
    "# Set hyperparameters and paths\n",
    "# --------------------------------------------------------------------------------\n",
    "img_rows = 256\n",
    "img_cols = 256\n",
    "channels = 3\n",
    "num_classes = 1  # Binary segmentation (cloud vs. non-cloud)\n",
    "epochs = 100\n",
    "train_batch_size = 12\n",
    "val_batch_size = 17\n",
    "\n",
    "train_dir = 'combined/train'\n",
    "test_dir = 'combined/test'\n",
    "\n",
    "def load_images_and_masks(root_dir):\n",
    "    \"\"\"\n",
    "    Loads images and masks from the given directories, \n",
    "    resizes them, and normalizes pixel values.\n",
    "    \"\"\"\n",
    "    # Lists to store image and mask file paths\n",
    "    images = []\n",
    "    masks = []\n",
    "\n",
    "    # Loop through files in the train directory\n",
    "    for filename in sorted(os.listdir(root_dir)):  # Sorting ensures image-mask alignment\n",
    "        file_path = os.path.join(root_dir, filename)\n",
    "\n",
    "        if filename.endswith(\".jpg\"):  # Check if it's an image\n",
    "            image = file_path\n",
    "            images.append(image)\n",
    "        elif filename.endswith(\"_mask.png\"):  # Check if it's a mask\n",
    "            mask = file_path\n",
    "            masks.append(mask)\n",
    "\n",
    "    return images, masks\n",
    "\n",
    "train_images, train_masks = load_images_and_masks(train_dir)\n",
    "val_images, val_masks = load_images_and_masks(test_dir)\n",
    "\n",
    "# Print the counts for verification\n",
    "print(f\"Number of training images: {len(train_images)}\")\n",
    "print(f\"Number of training masks: {len(train_masks)}\")\n",
    "print(f\"Number of validation images: {len(val_images)}\")\n",
    "print(f\"Number of validation masks: {len(val_masks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps_per_epoch = int(len(train_images) / train_batch_size)\n",
    "val_steps_per_epoch = int(len(val_images) / val_batch_size)\n",
    "train_steps_per_epoch, val_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "def data_generator(imglist, maplist, batchsize, size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Simplified data generator for binary classification problems without augmentations.\n",
    "    \n",
    "    Args:\n",
    "        imglist (list): List of image file paths.\n",
    "        maplist (list): List of corresponding mask file paths.\n",
    "        batchsize (int): Number of images per batch.\n",
    "        size (tuple): Target size for resizing (height, width).\n",
    "        \n",
    "    Yields:\n",
    "        img_batch (np.array): Batch of preprocessed images.\n",
    "        mask_batch (np.array): Batch of corresponding masks.\n",
    "    \"\"\"\n",
    "    assert len(imglist) == len(maplist), \"Mismatch between image and mask counts!\"\n",
    "    \n",
    "    h, w = size\n",
    "    while True:\n",
    "        # Initialize batch arrays\n",
    "        img_batch = np.zeros((batchsize, h, w, 3), dtype=np.float32)  # For RGB images\n",
    "        mask_batch = np.zeros((batchsize, h, w, 1), dtype=np.float32)  # For binary masks\n",
    "        \n",
    "        # Randomly sample indices for the batch\n",
    "        indices = np.random.choice(len(imglist), batchsize, replace=False)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            # Load image and mask\n",
    "            img = cv.imread(imglist[idx], cv.IMREAD_COLOR)  # Load as RGB\n",
    "            mask = cv.imread(maplist[idx], cv.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "            \n",
    "            # Resize to target size\n",
    "            img = cv.resize(img, (w, h), interpolation=cv.INTER_CUBIC)\n",
    "            mask = cv.resize(mask, (w, h), interpolation=cv.INTER_NEAREST)\n",
    "            \n",
    "            # Normalize image and scale mask\n",
    "            img_batch[i] = img / 255.0  # Normalize to [0, 1]\n",
    "            mask_batch[i] = np.expand_dims(mask / 255.0, axis=-1)  # Binary masks scaled to [0, 1]\n",
    "        \n",
    "        yield img_batch, mask_batch\n",
    "\n",
    "\n",
    "# Corrected helper function to estimate FLOPS\n",
    "def estimate_flops(model):\n",
    "    total_flops = 0\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            # Accessing the input shape via the input tensor\n",
    "            input_shape = layer.input.shape\n",
    "            flops = np.prod(input_shape[1:]) * layer.filters * np.prod(layer.kernel_size)\n",
    "            total_flops += flops\n",
    "        elif isinstance(layer, tf.keras.layers.Dense):\n",
    "            # Accessing the input shape via the input tensor\n",
    "            input_shape = layer.input.shape\n",
    "            flops = np.prod(input_shape[1:]) * layer.units\n",
    "            total_flops += flops\n",
    "    return total_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = data_generator(\n",
    "    train_images,\n",
    "    train_masks,\n",
    "    train_batch_size,\n",
    "    size=(img_rows, img_cols)\n",
    "    \n",
    ")\n",
    "\n",
    "validation_generator = data_generator(\n",
    "    val_images,\n",
    "    val_masks,\n",
    "    val_batch_size,\n",
    "    size=(img_rows, img_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Build and compile the model\n",
    "# --------------------------------------------------------------------------------\n",
    "input_shape = (\n",
    "    (img_rows, img_cols, 3) \n",
    "    if K.image_data_format() == 'channels_last' \n",
    "    else (3, img_rows, img_cols)\n",
    ")\n",
    "input_tensor = Input(shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model architecture\n",
    "model_name = 'unet3'\n",
    "if model_name == 'sq-unet':\n",
    "    model = SqueezeUNet(inputs=input_tensor, num_classes=num_classes, dropout=0.5, activation='sigmoid')\n",
    "elif model_name == 'see_unet':\n",
    "    model = SEE_Unet(inputs=input_tensor, num_classes=num_classes, dropout=0.5, activation='sigmoid')\n",
    "elif model_name == 'unet3':\n",
    "    model = unet(input_tensor)  \n",
    "\n",
    "model_file = f'{model_name}.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "# Custom callback to log learning rate\n",
    "class LearningRateLogger(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Access the learning rate\n",
    "        lr = float(K.get_value(self.model.optimizer.learning_rate))\n",
    "        logs['learning_rate'] = lr  # Add learning rate to logs\n",
    "        print(f\"\\nEpoch {epoch + 1}: Learning rate is {lr:.6f}\")\n",
    "\n",
    "# Model checkpoint configuration\n",
    "model_file = f'{model_name}.keras'\n",
    "\n",
    "# Log directory\n",
    "log_dir = \"logs/{}/\".format(model_name)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "# Load checkpoint if found\n",
    "if os.path.exists(model_file):\n",
    "    print(f\"Loading weights from checkpoint: {model_file}\")\n",
    "    model.load_weights(model_file)\n",
    "else:\n",
    "    print(\"No checkpoint found. Starting training from scratch.\")\n",
    "\n",
    "# Define model checkpoint\n",
    "cp = ModelCheckpoint(model_file, save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "# Early stopping configuration\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=100,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Learning rate schedule\n",
    "# lr_schedule = ExponentialDecay(initial_learning_rate=1e-3, decay_steps=10000, decay_rate=0.9)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "csvlogger = CSVLogger(os.path.join(log_dir, \"training.log\"), separator=',', append=True)\n",
    "lr_logger = LearningRateLogger()  # Instantiate the learning rate logger\n",
    "\n",
    "start = time.time()\n",
    "# Train the model\n",
    "model_history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=val_steps_per_epoch,\n",
    "    callbacks=[cp, csvlogger, early_stopping, lr_logger]\n",
    ")\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Time (s)', end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to the CSVLogger file\n",
    "log_dir = \"logs/{}/\".format(model_name)\n",
    "log_file = os.path.join(log_dir, \"training.log\")\n",
    "\n",
    "# Check if the log file exists\n",
    "if not os.path.exists(log_file):\n",
    "    raise FileNotFoundError(f\"No log file found at {log_file}. Ensure training has been performed and logs are saved.\")\n",
    "\n",
    "# Load the CSVLogger file\n",
    "log_data = pd.read_csv(log_file)\n",
    "\n",
    "# Extract the epoch, training loss, and validation loss\n",
    "epochs = log_data['epoch']\n",
    "training_loss = log_data['loss']\n",
    "validation_loss = log_data['val_loss']\n",
    "\n",
    "# Plot the loss graph\n",
    "plt.figure()\n",
    "plt.plot(epochs, training_loss, 'r', label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, 'bo', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])  # Adjust limits based on your data\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from keras.models import load_model\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_file = f'{model_name}.keras'\n",
    "log_dir = \"logs\"\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "seed = 42\n",
    "# os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "def set_seeds(seed=seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# Activate Tensorflow deterministic behavior\n",
    "def set_global_determinism(seed=seed):\n",
    "    set_seeds(seed=seed)\n",
    "\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    \n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "\n",
    "# Call the above function with seed value\n",
    "set_global_determinism(seed=seed)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Constants\n",
    "# --------------------------------------------------------------------------------\n",
    "model_file = f'{model_name}.keras'\n",
    "\n",
    "checkpoint_path = model_file\n",
    "threshold = 0.5\n",
    "# model.load_weights(checkpoint_path)\n",
    "# --------------------------------------------------------------------------------\n",
    "# Load the best model after training\n",
    "# --------------------------------------------------------------------------------\n",
    "print(\"Loading the best model...\")\n",
    "model = load_model(checkpoint_path, safe_mode=False)\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Generate predictions and measure latency\n",
    "# --------------------------------------------------------------------------------\n",
    "start_time = time.time()\n",
    "val_predictions = model.predict(validation_generator, steps=val_steps_per_epoch)\n",
    "end_time = time.time()\n",
    "\n",
    "inference_time = end_time - start_time\n",
    "num_val_samples = len(val_images)\n",
    "average_latency = inference_time / num_val_samples\n",
    "\n",
    "print(f\"Number of validation predictions: {len(val_predictions)}\")\n",
    "print(f\"Inference time for all validation samples: {inference_time:.2f} seconds\")\n",
    "print(f\"Average latency per image: {average_latency:.4f} seconds\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Binarize predictions\n",
    "# --------------------------------------------------------------------------------\n",
    "val_predictions_binary = (val_predictions > threshold).astype(np.uint8)\n",
    "print(f\"Number of validation predictions (binary): {len(val_predictions_binary)}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Evaluate model on validation set\n",
    "# --------------------------------------------------------------------------------\n",
    "loss, accuracy = model.evaluate(validation_generator, steps=val_steps_per_epoch)\n",
    "print(f\"Validation Loss: {loss}\")\n",
    "print(f\"Validation Accuracy: {accuracy}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Calculate additional metrics\n",
    "# --------------------------------------------------------------------------------\n",
    "precision_scores, recall_scores, f1_scores, accuracy_scores, dice_scores = [], [], [], [], []\n",
    "\n",
    "# Reset the generator if necessary\n",
    "validation_generator = data_generator(\n",
    "    val_images,\n",
    "    val_masks,\n",
    "    val_batch_size,\n",
    "    size=(img_rows, img_cols)\n",
    ")\n",
    "\n",
    "output_dir = os.path.join(\"predictions\", model_name)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Process each batch\n",
    "for i in tqdm(range(val_steps_per_epoch)):\n",
    "    imgs, masks = next(validation_generator)\n",
    "    predictions = model.predict(imgs)\n",
    "\n",
    "    predictions_flat = (predictions > threshold).astype(int).flatten()\n",
    "    masks_flat = (masks > threshold).astype(int).flatten()\n",
    "\n",
    "    precision_scores.append(precision_score(masks_flat, predictions_flat, zero_division=0))\n",
    "    recall_scores.append(recall_score(masks_flat, predictions_flat, zero_division=0))\n",
    "    f1_scores.append(f1_score(masks_flat, predictions_flat, zero_division=0))\n",
    "    accuracy_scores.append(accuracy_score(masks_flat, predictions_flat))\n",
    "\n",
    "    intersection = np.sum(masks_flat * predictions_flat)\n",
    "    dice_score =  (2. * intersection) / (np.sum(masks_flat) + np.sum(predictions_flat) + 1e-7)\n",
    "    dice_scores.append(dice_score)\n",
    "\n",
    "    # Save images\n",
    "    for j in range(len(imgs)):\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "        ax[0].imshow(imgs[j], cmap='gray')\n",
    "        ax[0].set_title(\"Input Image\")\n",
    "        ax[0].axis(\"off\")\n",
    "\n",
    "        ax[1].imshow(masks[j].squeeze(), cmap='gray')\n",
    "        ax[1].set_title(\"Ground Truth Mask\")\n",
    "        ax[1].axis(\"off\")\n",
    "\n",
    "        ax[2].imshow(predictions[j].squeeze() > threshold, cmap='gray')\n",
    "        ax[2].set_title(\"Predicted Mask\")\n",
    "        ax[2].axis(\"off\")\n",
    "\n",
    "        # Save the figure\n",
    "        plt.savefig(os.path.join(output_dir, f\"batch_{i}_sample_{j}.png\"), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "error_rate = 1 - avg_accuracy\n",
    "dice_score = np.mean(dice_scores)\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"Average Error Rate: {error_rate:.4f}\")\n",
    "print(f\"Average Dice Score: {dice_score:.4f}\")\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Measure the model size in MB\n",
    "# --------------------------------------------------------------------------------\n",
    "model_size = os.path.getsize(checkpoint_path) / (1024 * 1024)\n",
    "print(f\"Model size: {model_size:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
